\chapter{Δίκτυα αραιής ενεργοποίησης}
\label{chapter6}
\graphicspath{{./images/sparsely-activated-networks/}}

\newcommand\figscale{0.16}
\tdplotsetmaincoords{100}{-70}
\DTLloaddb{keys_values}{keys_values.csv}

\section{Εισαγωγή}
Τα DNN~\cite{lecun2015deep} χρησιμοποιούν πολλαπλά στοιβαγμένα επίπεδα με βάρη και συναρτήσεις ενεργοποίησης, που μετασχηματίζουν την είσοδο σε ενδιάμεσες αναπαραστάσεις κατά τη διάρκεια της προς-τα-εμπρός διάδοσης.
Χρησιμοποιώντας backpropagation~\cite{rumelhart1986learning} η κλίση του κάθε βάρους σε σχέση με το σφάλμα της εξόδου υπολογίζεται και μεταβιβάζεται σε μια συνάρτηση βελτιστοποίησης όπως η Στοχαστική Κάθοδος Κλίσεων ή Adam~\cite{kingma2014adam} η οποία ενημερώνει τις τιμές των βαρών κάνοντας την έξοδο του δικτύου να συγκλίνει στην επιθυμητή έξοδο.
Τα DNN χρησιμοποιώντας μεγάλο όγκο δεδομένων και ισχυρές μονάδες παράλληλης επεξεργασίας έχουν επιτύχει υψηλού επιπέδου αποτελέσματα σε προβλήματα όπως η αναγνώριση εικόνας~\cite{krizhevsky2012imagenet} και ομιλίας~\cite{graves2013speech}.
Ωστόσο, αυτές οι καινοτομίες έχουν συμβεί σε βάρος της αύξησης του μήκους περιγραφής των αναπαραστάσεων, οι οποίες στα DNN είναι ανάλογες με τον αριθμό των:
\begin{enumerate}
	\item βαρών του μοντέλου και
	\item μη-μηδενικών ενεργοποιήσεων.
\end{enumerate}

Η χρήση μεγάλου αριθμού βαρών ως επιλογή σχεδιασμού σε αρχιτεκτονικές όπως το Inception~\cite{szegedy2016rethinking}, VGGnet~\cite{simonyan2014very} και ResNet~\cite{he2016deep} (συνήθως με την αύξηση του βάθους), ακολουθήθηκε από έρευνα που επέδειξε τον πλεονασμό των βαρών των DNNs.
Αποδείχθηκε ότι τα DNN προσαρμόζονται εύκολα σε τυχαίες επισημάνσεις δεδομένων~\cite{zhang2016understanding} και ότι σε κάθε αρχικοποιημένο DNN υπάρχει ένα υποδίκτυο που μπορεί να επιλύσει το δεδομένο πρόβλημα, με την ίδια ακρίβεια με το αρχικά εκπαιδευμένο~\cite{frankle2018lottery}.

Επιπλέον τα DNN με μεγάλο αριθμό βαρών έχουν υψηλότερες απαιτήσεις αποθήκευσης και είναι πιο αργά κατά τη διάρκεια των συμπερασμών.
Προηγούμενη έρευνα που έγινε πάνω σε αυτό το πρόβλημα, επικεντρώθηκε στο κλάδεμα βαρών από εκπαιδευμένα DNNs~\cite{aghasi2017net} και κλάδεμα βαρών κατά τη διάρκεια της εκπαίδευσης~\cite{lin2017runtime}.
Το κλάδεμα ελαχιστοποιεί την χωρητικότητα του μοντέλου για χρήση σε περιβάλλοντα με χαμηλές υπολογιστικές δυνατότητες ή χαμηλές απαιτήσεις χρόνου συμπερασμών και επίσης βοηθά στην μείωση της προσαρμογής των νευρώνων η οποία επίσης έχει αντιμετωπιστεί από το Dropout~\cite{srivastava2014dropout}.
Ωστόσο οι στρατηγικές κλαδέματος λαμβάνουν υπόψη μόνο τον αριθμό των βαρών του μοντέλου.

Το άλλο στοιχείο που επηρεάζει το μήκος περιγραφής των αναπαραστάσεων των DNN, είναι ο αριθμός των μη-μηδενικών ενεργοποιήσεων στις ενδιάμεσες αναπαραστάσεις που σχετίζονται με την έννοια της αραιότητας.
Στα νευρωνικά δίκτυα η αραιότητα μπορεί να εφαρμοστεί είτε στις συνδέσεις μεταξύ νευρώνων είτε στους χάρτες ενεργοποίησης~\cite{laughlin2003communication}.
Παρόλο που η αραιότητα στους χάρτες ενεργοποίησης συνήθως επιβάλλεται στη συνάρτηση απώλειας με την προσθήκη ενός όρου κανονικοποίησης $L_{1, 2}$ ή απόκλισης Kullback-Leibler~\cite{kingma2013auto}, θα μπορούσαμε επίσης να επιτύχουμε αραιότητα στους χάρτες ενεργοποίησης με τη χρήση μιας κατάλληλης συνάρτησης ενεργοποίησης.

Αρχικά, χρησιμοποιήθηκαν φραγμένες συναρτήσεις όπως η σιγμοειδής και η υπερβολική εφαπτομένη ($\tanh$), αλλά εκτός από την παραγωγή πυκνών χαρτών ενεργοποίησης παρουσιάζουν επίσης το πρόβλημα του vanishing gradients~\cite{bengio1994learning}.
Το ReLU προτάθηκε αργότερα~\cite{glorot2011deep, nair2010rectified} ως μια συνάρτηση ενεργοποίησης που λύνει το πρόβλημα της διαφυγής των κλίσεων και αυξάνει την αραιότητα των χαρτών ενεργοποίησης.
Αν και το ReLU δημιουργεί μηδενικά (σε αντίθεση με τους προκατόχους του σιγμοειδής και $\tanh$), ο χάρτης ενεργοποίησής του αποτελείται από αραιά χωρισμένες αλλά πυκνές περιοχές (Εικ.~\ref{fig:activationfunctions}\subref{subfig:relu}) αντί για αραιές αιχμές.
Το ίδιο ισχύει για άλλες γενικεύσεις του ReLU, όπως το Παραμετρικό ReLU~\cite{he2015delving} και το Maxout~\cite{goodfellow2013maxout}.
Πρόσφατα, στο $k$-Sparse Autoencoders~\cite{makhzani2013k} χρησιμοποιήθηκε μια συνάρτηση ενεργοποίησης που εφαρμόζει κατωφλίωση μέχρι να παραμείνουν οι $k$ μεγαλύτερες ενεργοποιήσεις, ωστόσο αυτή η μη-γραμμικότητα καλύπτει μια περιορισμένη περιοχή του χάρτη ενεργοποίησης δημιουργώντας αραιά αποσυνδεδεμένες πυκνές περιοχές (Εικ.~\ref{fig:activationfunctions}\subref{subfig:topk_absolutes}), παρόμοιες με την περίπτωση ReLU\@.

\begin{sidewaysfigure}
	\centering
	\subfloat{\includegraphics[width=0.2\textwidth]{"images_1d/UCI-epilepsy_identity_1d_2_activations_0"}}
	\subfloat{\includegraphics[width=0.2\textwidth]{"images_1d/UCI-epilepsy_relu_1d_2_activations_0"}}
	\subfloat{\includegraphics[width=0.2\textwidth]{"images_1d/UCI-epilepsy_topk_absolutes_1d_2_activations_0"}}
	\subfloat{\includegraphics[width=0.2\textwidth]{"images_1d/UCI-epilepsy_extrema_pool_indices_1d_2_activations_0"}}
	\subfloat{\includegraphics[width=0.2\textwidth]{"images_1d/UCI-epilepsy_extrema_1d_2_activations_0"}}
	\\
	\setcounter{subfigure}{0}
	\subfloat[Identity]{\includegraphics[width=0.2\textwidth]{"images_2d/MNIST_identity_2d_2_activations_1"}\label{subfig:identity}}
	\subfloat[ReLU]{\includegraphics[width=0.2\textwidth]{"images_2d/MNIST_relu_2d_2_activations_0"}\label{subfig:relu}}
	\subfloat[top-k absolutes]{\includegraphics[width=0.2\textwidth]{"images_2d/MNIST_topk_absolutes_2d_2_activations_0"}\label{subfig:topk_absolutes}}
	\subfloat[Extrema-Pool indices]{\includegraphics[width=0.2\textwidth]{"images_2d/MNIST_extrema_pool_indices_2d_2_activations_0"}\label{subfig:extremapoolindices}}
	\subfloat[Extrema]{\includegraphics[width=0.2\textwidth]{"images_2d/MNIST_extrema_2d_2_activations_0"}\label{subfig:extrema}}
	\caption[Οπτικοποίηση χαρτών ενεργοποίησης πέντε συναρτήσεων αραιής ενεργοποίησης (Identity, ReLU, top-k absolutes, Extrema-Pool indices και Extrema) για 1D και 2D είσοδο.]{Οπτικοποίηση πέντε συναρτήσεων αραιής ενεργοποίησης (Identity, Max-Activations, Max-Pool indices και Extrema) για 1D και 2D είσοδο στην πρώτη και τη δεύτερη σειρά αντίστοιχα.
	Η 1D είσοδος στις συναρτήσεις ενεργοποίησης υποδηλώνεται με τη συνεχή διαφανή πράσινη γραμμή, χρησιμοποιώντας ένα παράδειγμα από τη βάση δεδομένων επιληψίας του UCI\@.
	Η έξοδος κάθε συνάρτησης ενεργοποίησης απεικονίζεται με τις κυανές γραμμές με τους μπλε δείκτες.
	Το 2D παράδειγμα απεικονίζει μόνο την έξοδο των συναρτήσεων ενεργοποίησης, χρησιμοποιώντας ένα παράδειγμα από τη βάση δεδομένων MNIST\@.
	}
	\label{fig:activationfunctions}
\end{sidewaysfigure}

Επιπλέον συναρτήσεις ενεργοποίησης που παράγουν συνεχείς χάρτες ενεργοποίησης (όπως το ReLU) είναι λιγότερο βιολογικά εύλογες, επειδή οι βιολογικοί νευρώνες σπάνια βρίσκονται στο μέγιστο επίπεδο κορεσμού τους~\cite{bush1996inhibition} και επίσης χρησιμοποιούν αιχμές για να επικοινωνούν αντί για συνεχείς τιμές~\cite{bengio2015towards}.
Προηγούμενη βιβλιογραφία έχει επίσης καταδείξει την αυξημένη βιολογική ευλογοφάνεια της αραιότητας στα τεχνητά νευρωνικά δίκτυα~\cite{rehn2007network}.
Η αραιότητα επιπέδου αιχμών στους χάρτες ενεργοποίησης έχει ερευνηθεί διεξοδικά στα βιολογικώς πιο αποδεκτά μοντέλα που επικοινωνούν με βάση το ρυθμό (rate-based)~\cite{heiberg2018firing}, αλλά δεν έχει διερευνηθεί διεξοδικά ως επιλογή σχεδιασμού για συναρτήσεις ενεργοποίησης σε συνδυασμό με συνελικτικά φίλτρα.

Ο αυξημένος αριθμός βαρών και των μη-μηδενικών ενεργοποιήσεων καθιστούν τα DNNs  πολύπλοκα και έτσι είναι δυσκολότερο να χρησιμοποιηθούν σε προβλήματα που απαιτούν την αντιστοίχηση αιτιότητας της εξόδου με ένα συγκεκριμένο σύνολο νευρώνων.
Η πλειοψηφία των τομέων που κάνουν χρήση της μηχανικής μάθησης συμπεριλαμβανομένων τομέων όπως η υγειονομική περίθαλψη~\cite{bizopoulos2019deep} απαιτούν τα μοντέλα να είναι ερμηνεύσιμα και εξηγήσιμα πριν θεωρηθούν ως πιθανή λύση.
Αν και αυτές οι ιδιότητες μπορούν να αυξηθούν χρησιμοποιώντας ανάλυση ευαισθησίας~\cite{simonyan2013deep}, μεθόδους αποσυνέλιξης~\cite{zeiler2014visualizing}, Layerwise-Relevance Propagation~\cite{bach2015pixel} και Local-Interpretable Model agnostic Explanations~\cite{ribeiro2016should} θα ήταν προτιμότερο να έχουμε αυτο-ερμηνεύσιμα μοντέλα.

Επιπλέον λαμβάνοντας υπόψη ότι τα DNNs μαθαίνουν να αναπαριστούν δεδομένα χρησιμοποιώντας το συνδυαστικό σύνολο των βαρών και των μη-μηδενικών ενεργοποιήσεων κατά τη διάρκεια της προς-τα-εμπρός διάδοσης, προκύπτει ένα ενδιαφέρον ερώτημα:
\\\\
\indent\textit{Ποιες είναι οι επιπτώσεις του συμβιβασμού μεγαλύτερου σφάλματος ανακατασκευής των αναπαραστάσεων και του λόγου συμπίεσης των αναπαραστάσεων σε σχέση με στα αρχικά δεδομένα;}
\\

Προηγούμενη εργασία από τους Blier et al.~\cite{blier2018description} έδειξε την ικανότητα των DNN να συμπιέζουν χωρίς απώλειες τα δεδομένα εισόδου και τα βάρη, αλλά χωρίς να παίρνουν υπόψη τον αριθμό των μη-μηδενικών ενεργοποιήσεων.
Σε αυτό το έργο, χαλαρώνουμε την απαίτηση της μη-απώλειας και επίσης θεωρούμε τα νευρωνικά δίκτυα καθαρά ως προσεγγιστές συναρτήσεων, αντί για πιθανοτικά μοντέλα.
Οι συνεισφορές του παρόντος είναι οι ακόλουθες προτάσεις:
\begin{itemize}
	\item Το μέτρο $\varphi$ που αξιολογεί μη-επιβλεπώμενα μοντέλα με βάση το πόσο συμπιεσμένες είναι οι αναπαραστάσεις σε σχέση με τα αρχικά δεδομένα και κατα πόσο ακριβή είναι στις ανακατασκευές τους.
	\item Δίκτυα Αραιής Ενεργοποίησης (SANs) (Εικ.~\ref{fig:sans}), στα οποία επιβάλλεται αραιότητα αιχμών στον χάρτη ενεργοποίησης μέσω μιας συνάρτησης αραιής ενεργοποίησης (Εικ.~\ref{fig:activationfunctions}\subref{subfig:extremapoolindices} και \subref{subfig:extrema}).
\end{itemize}

\section{Μέτρο $\varphi$}
\label{sec6:flethos}
Έστω $M$ ένα μοντέλο με $q$ πυρήνες κάθε ένας από τους οποίους με $m^{(i)}$ δείγματα και έστω $\mathcal{L}$ η συνάρτηση απώλειας ανακατασκευής:
\begin{equation}
	\label{eq:model}
	M: \bm{x} \longmapsto \hat{\bm{x}}
\end{equation}

\noindent
, όπου $\bm{x} \in \mathbb{R}^n$ είναι το διάνυσμα εισόδου και $\hat{\bm{x}}$ είναι η ανακατασκευή του $\bm{x}$.
Για τον ορισμό του μέτρου $\varphi$ χρησιμοποιούμε ένα νευρωνικό δίκτυο το οποίο περιέχει συνελικτικά φίλτρα, παρ'όλ'αυτά ο συγκεκριμένος ορισμός μπορεί να γενικευτεί και σε άλλες αρχιτεκτονικές.
Το μέτρο $\varphi$ αξιολογεί ένα μοντέλο με βάση δύο έννοιες: την `φλυαρία' (verbosity) και την ακρίβειά του.

Η `φλυαρία' στα νευρωνικά δίκτυα μπορεί να γίνει αντιληπτή ως αντιστρόφως ανάλογη με το λόγο συμπίεσης των αναπαραστάσεων.
Πρώτον, υπολογίζουμε τον αριθμό των βαρών $W$ ενός μοντέλου $M$ ως εξής:
\begin{equation}
	\label{eq:numberofweights}
	W = \sum\limits_{i=1}^q m^{(i)}
\end{equation}

Επίσης υπολογίζουμε τον αριθμό των μη-μηδενικών ενεργοποιήσεων $A$ ενός μοντέλου $M$ για είσοδο $\bm{x}$ ως:
\begin{equation}
	\label{eq:numberofactivations}
	A_{\bm{x}} = \sum\limits_{i=1}^q \Big\lVert\bm{\alpha}^{(i)}\Big\lVert_0
\end{equation}

\noindent
, όπου $\lVert \cdot \rVert_0$ δηλώνει την ψευδο-νόρμα $\ell_0$ και $\bm{\alpha}^{(i)}$ τον χάρτη ενεργοποίησης του $i^{th}$ πυρήνα.
Στη συνέχεια, χρησιμοποιώντας τις Εξισώσεις~\ref{eq:numberofweights} και~\ref{eq:numberofactivations} ορίζουμε τη σχέση συμπίεσης $CR$ του $\bm{x}$ σε σχέση με $M$ ως:
\begin{equation}
	\label{eq:compressionratio}
	CR = \frac{n}{W + (\dim(\bm{x}) + 1)A_{\bm{x}}}
\end{equation}

\noindent
, όπου $\dim$ ορίζει την διαστασιμότητα.
Ο λόγος που πολλαπλασιάζουμε τη διαστασιμότητα του $\bm{x}$ με τον αριθμό των ενεργοποιήσεων $A_{\bm{x}}$, είναι ότι πρέπει να εξετάσουμε τη χωρική θέση κάθε μη-μηδενικής ενεργοποίησης εκτός από το πλάτος του για την ανακατασκευή $\bm{x}$.
Επιπλέον, χρησιμοποιώντας αυτόν τον ορισμό του $CR$ δημιουργείται ένας επιθυμητός συμβιβασμός μεταξύ της χρήσης ενός μεγαλύτερου πυρήνα με λιγότερες εμφανίσεις και ενός μικρότερου πυρήνα με περισσότερες εμφανίσεις, βάσει του οποίου αποφασίζεται το μέγεθος του πυρήνα που ελαχιστοποιεί το $CR$.

Όσον αφορά την ακρίβεια ορίζουμε την κανονικοποιημένη απώλεια ανακατασκευής ως εξής:
\begin{equation}
	\label{eq:normalizedreconstrucionloss}
	\tilde{\mathcal{L}}(\hat{\bm{x}},\bm{x}) = \frac{\mathcal{L}(\hat{\bm{x}},\bm{x})}{\mathcal{L}(0,\bm{x})}
\end{equation}

Τέλος, χρησιμοποιώντας τις Εξισώσεις~\ref{eq:compressionratio} και\ref{eq:normalizedreconstrucionloss} ορίζουμε το μέτρο $\varphi$\footnote{Η χρήση του συμβόλου $\varphi$ προέρχεται από τον πρώτο χαρακτήρα του σύνθετου ελληνικού ουσιαστικού `φλύθος' = φλύ + θος. Αποτελείται από το πρώτο μέρος της λέξης φλύ-αρος και το δεύτερο μέρος της λέξης λά-θος. Το φλύθος ορίζεται κυριολεκτικά ως:\textit{Παροχή ανακριβών πληροφοριών χρησιμοποιώντας πολλές λέξεις. Η κατάσταση του να είναι κανείς λάθος και φλύαρος ταυτόχρονα}.} του $\bm{x}$ σε σχέση με $M$ ως εξής:
\begin{equation}
	\label{eq:flethos}
	\varphi = \lVert(CR^{-1}, \tilde{\mathcal{L}}(\hat{\bm{x}},\bm{x}))\rVert_2
\end{equation}

\noindent
, όπου $\lVert\cdot\rVert_2$ ορίζει την ευκλείδια απόσταση.

Όσον αφορά την επιλογή υπερπαραμέτρων, ορίζουμε επίσης το μέτρο $\varphi$ ενός συνόλου δεδομένων ή μια μίνι-παρτίδας σε σχέση με το $M$ ως:
\begin{equation}
	\label{eq:meanflethos}
	\bar\varphi = \frac{1}{l} \sum \limits_{j=1}^l \varphi^{(j)}
\end{equation}

\noindent
, όπου $l$ είναι ο αριθμός παρατηρήσεων της βάσης δεδομένων ή το μέγεθος της παρτίδας.

Το $\bar\varphi$ είναι μη διαφοροποιήσιμο λόγω της παρουσίας της ψευδο-νόρμας $\ell_0$ στην Εξ.~\ref{eq:numberofactivations}.
Ένας τρόπος για να ξεπεραστεί αυτό το πρόβλημα, είναι η χρήση του $\mathcal{L}$ ως τη διαφοροποιήσιμη συνάρτηση βελτιστοποίησης κατά τη διάρκεια της εκπαίδευσης και η χρήση του $\bar\varphi$ ως το μέτρο για την επιλογή μοντέλου κατά την επικύρωση, κατά την διάρκεια της οποίας παίρνονται οι αποφάσεις για τις επιλογές των τιμών των υπερπαραμέτρων (όπως το μέγεθος του πυρήνα).

\section{Δίκτυα αραιής ενεργοποίησης}
\label{sec6:sans}

\subsection{Συναρτήσεις αραιής ενεργοποίησης}
\label{sec6:safs}
Σε αυτή την υποενότητα ορίζουμε πέντε συναρτήσεις ενεργοποίησης $\phi$ και την αντίστοιχη παράμετρο αραιότητας πυκνότητας $d^{(i)}$ για τα οποία έχουμε:
\begin{equation}
	\label{eq:phi}
	\phi: s \longmapsto \alpha
\end{equation}

Επιλέγουμε τιμές για το $d^{(i)}$ για κάθε συνάρτηση αραιής ενεργοποίησης έτσι ώστε να έχουμε περίπου τον ίδιο αριθμό ενεργοποιήσεων για να έχουμε δίκαιη σύγκριση μεταξύ των συναρτήσεων αραιής ενεργοποίησης (εκτός από την Identity, η οποία δεν έχει παράμετρο αραιότητας).

\subsubsection{Ταυτότητα}
\label{sec6:identity}
$\phi = \mathbbm{1}$.
Η συνάρτηση ενεργοποίησης ταυτότητας (Identity) χρησιμεύει ως βάση αναφοράς και διατηρεί ακέραια την είσοδό του, όπως φαίνεται στην Εικ.~\ref{fig:activationfunctions}\subref{subfig:identity}.
Για αυτήν την περίπτωση, δεν εφαρμόζεται παράμετρος αραιότητας $d^{(i)}$.

\subsubsection{ReLU}
\label{sec6:relu}
$\phi = ReLU(s)$.
Η συνάρτηση ενεργοποίησης ReLU παράγει αραιά συνδεδεμένες αλλά πυκνές περιοχές, όπως φαίνεται στην Εικ.~\ref{fig:activationfunctions}\subref{subfig:relu}.
Για αυτήν την περίπτωση, δεν εφαρμόζεται παράμετρος αραιότητας $d^{(i)}$.

\subsubsection{κ-μέγιστα απολύτων}
\label{sec6:topk_absolutes}
Η συνάρτηση κ-μεγίστων απολύτων (top-k absolutes) (που ορίζεται στον αλγόριθμο~\ref{alg:topk_absolutes}) διατηρεί τους δείκτες των $k$ ενεργοποιήσεων με τη μεγαλύτερη απόλυτη τιμή και μηδενίζει τις υπόλοιπες, όπου $1 \le k <n \in \mathbb{N}$.
Ορίσαμε $d^{(i)} = k$, όπου $k = \lfloor n / m \rfloor^{\dim(\bm{x})}$.
Η συνάρτηση ενεργοποίησης απόλυτων κ-μεγίστων είναι πιο αραιή από το ReLU αλλά κάποια ακρότατα ενεργοποιούνται πολλαπλά σε σχέση με άλλα ακρότατα που δεν ενεργοποιούνται καθόλου, όπως φαίνεται στην Εικ.~\ref{fig:activationfunctions}\subref{subfig:topk_absolutes}.

\begin{algorithm}[H]
	\caption{top-k absolutes}
	\label{alg:topk_absolutes}
	\input{chapter6_topk_absolutes.tex}
\end{algorithm}

\subsubsection{Δείκτες Συγκέντρωσης Ακρότατων}
\label{sec6:extremapoolindices}
Η συνάρτηση ενεργοποίησης των δεικτών συγκέντρωσης ακρότατων (Extrema-Pool indices) (που ορίζεται στον Αλγόριθμο~\ref{alg:extremapoolindices}) διατηρεί μόνο τον δείκτη της ενεργοποίησης με το μέγιστο απόλυτο πλάτος από κάθε περιοχή που περιγράφεται από ένα πλέγμα με την ίδια αναλυτικότητα όπως το μέγεθος πυρήνα $m^{(i)}$ και μηδενίζει τις υπόλοιπες.
Αποτελείται από ένα στρώμα μέγιστης συγκέντρωσης ακολουθούμενο από ένα στρώμα μέγιστης αποσυγκέντρωσης με τις ίδιες παραμέτρους, ενώ η παράμετρος αραιότητας $d^{(i)}$ σε αυτή την περίπτωση ορίζεται ως $d^{(i)} = m^{(i)} <n \in \mathbb{N}$.
Αυτή η συνάρτηση ενεργοποίησης δημιουργεί πιο αραιούς χάρτες ενεργοποίησης από τα απόλυτα κ-μέγιστα, παρ'όλ'αυτά σε περιπτώσεις που το πλέγμα της συγκέντρωσης είναι κοντά σε ακρότατο υπάρχει το ενδεχόμενο να ενεργοποιηθεί διπλά (όπως φαίνεται στην Εικ.~\ref{fig:activationfunctions}\subref{subfig:extremapoolindices}).

\begin{algorithm}[H]
	\caption{Δείκτες συγκέντρωσης ακρότατων}
	\label{alg:extremapoolindices}
	\input{chapter6_extremapoolindices.tex}
\end{algorithm}

\subsubsection{Ακρότατα}
\label{sec6:extrema}
Η συνάρτηση ενεργοποίησης ακρότατων (Extrema) (που ορίζεται στον Αλγόριθμο~\ref{alg:extrema}) ανιχνεύει τα υποψήφια ακρότατα χρησιμοποιώντας τη μηδενική διέλευση της πρώτης παραγώγου και στη συνέχεια τις ταξινομεί σε φθίνουσα σειρά και σταδιακά εξαλείφει εκείνες τα ακρότατα που έχουν μικρότερο πλάτος από ένα γειτονικό ακρότατο σύμφωνα με μια προκαθορισμένη απόσταση $med$.
Η επιβολή ελάχιστης απόστασης ακρότατων στον αλγόριθμο ανίχνευσης ακρότατων καθιστά την $\bm{\alpha}$ πιο αραιή από τις προηγούμενες περιπτώσεις και λύνει το πρόβλημα της διπλής ενεργοποίησης ακρότατων που εμφανίζουν οι δείκτες συγκέντρωσης ακρότατων (όπως φαίνεται στην Εικ.~\ref{fig:activationfunctions}\subref{subfig:extrema}).
Η παράμετρος αραιότητας σε αυτή την περίπτωση ορίζεται $d^{(i)} = med$, όπου $1 \le med <n \in \mathbb{N}$ είναι η ελάχιστη απόσταση ακρότατων.
Ορίσαμε $med = m^{(i)}$ για τη δίκαιη σύγκριση μεταξύ των συναρτήσεων αραιής ενεργοποίησης.
Ειδικά για την συνάρτηση ενεργοποίησης ακρότατων εισάγουμε μια παράμετρο `συνοριακής ανοχής' για να επιτρέψουμε μια πιο έγκαιρη ενεργοποίηση του νευρώνα.

\begin{algorithm}[H]
	\caption{Ανίχνευση ακρότατων με ελάχιστη απόσταση $med$}
	\label{alg:extrema}
	\scalebox{0.9}{
		\begin{minipage}{\linewidth}
			\input{chapter6_extrema.tex}
		\end{minipage}
		}
\end{algorithm}

\subsection{Αρχιτεκτονική και εκπαίδευση των SANs}

\begin{figure}
	\subfloat[1D SAN]{\begin{tikzpicture}[]
		\input{chapter6_san_1d.tex}
	\end{tikzpicture}
	}
	\qquad
	\subfloat[2D SAN]{\begin{tikzpicture}[]
		\input{chapter6_san_2d.tex}
	\end{tikzpicture}
	}
	\caption[Διαγράμματα της προς-τα-εμπρός διάδοσης ενός 1D και 2D SAN με δύο πυρήνες για παραδείγματα από την βάση δεδομένων επιληψίας UCI και MNIST αντίστοιχα.]{Διαγράμματα της προς-τα-εμπρός διάδοσης 1D και 2D SAN με δύο πυρήνες για παραδείγματα από την βάση δεδομένων επιληψίας UCI και MNIST αντίστοιχα.
	Οι εικόνες απεικονίζουν τις ενδιάμεσες αναπαραστάσεις; το $\bm{x}$ δηλώνει το σήμα εισόδου (μπλε γραμμή), το $\bm{w}^{(i)}$ τους πυρήνες (κόκκινη γραμμή), το $\bm{s}^{(i)}$ τους πίνακες ομοιότητας (πράσινη γραμμή), το $\bm{\alpha}{(i)}$ τους χάρτες ενεργοποίησης (κυανές γραμμές με μπλε δείκτες), το $\bm{r}^{(i)}$ την μερική ανακατασκευή από κάθε $\bm{w}^{(i)}$ και το $\hat{\bm{x}}$ την ανακατασκευασμένη είσοδο (κόκκινη γραμμή).
	Για λόγους σύγκρισης, η διαφανής πράσινη γραμμή στο $\bm{\alpha}^{(i)}$ δηλώνει το αντίστοιχο $\bm{s}^{(i)}$ και η διαφανή μπλε γραμμή στο $\hat{\bm{x}}$ δηλώνει την είσοδο $\bm{x}$.
	Ο εκθέτης $i = 0,1$ αντιστοιχεί στον πρώτο και στον δεύτερο πυρήνα και στις ενδιάμεσες αναπαραστάσεις.
	Οι κύκλοι δηλώνουν συναρτήσεις. Το $\mathcal{L}$ δηλώνει τη συνάρτηση απώλειας, το $\phi$ τη συνάρτηση αραιής ενεργοποίησης, το $\ast$ τη συνάρτηση συνέλιξης και το $+$ τον τελεστή πρόσθεσης.
	Όλες οι συναρτήσεις εκτελούνται ξεχωριστά για κάθε $\bm{w}^{(i)}$, ωστόσο για οπτική διαύγεια απεικονίζουμε μόνο μία συνάρτηση για κάθε βήμα.
	Αποχρώσεις του κόκκινου και του μπλε απεικονίζουν θετικές και αρνητικές τιμές αντίστοιχα.
	Η Peak συνάρτηση ενεργοποίησης χρησιμοποιήθηκε και για τα δύο δίκτυα.
	}
	\label{fig:sans}
\end{figure}

Έστω $\bm{x} \in \mathbb{R}^n$ είναι ένα δεδομένο εισόδου, ωστόσο τα παρακάτω μπορεί να γενικευτούν σε εισόδους παρτίδων με διαφορετικά μήκη.
Έστω $\bm{w}^{(i)} \in \mathbb{R}^{m^{(i)}}$ ο πίνακας βάρους του $i^{th}$ πυρήνα ο οποίος αρχικοποιείται χρησιμοποιώντας μια κανονική κατανομή με μέση τιμή $\mu$ και τυπική απόκλιση $\sigma$:
\begin{equation}
	\label{eq:weightinitialization}
	\bm{w}^{(i)} \sim \mathcal{N}(\mu, \sigma)
\end{equation}

\noindent
, όπου $0 \le i <q \in \mathbb{N}$ είναι ο αριθμός των πυρήνων.

Αρχικά υπολογίζουμε τους πίνακες ομοιότητας\footnote{Η προηγούμενη βιβλιογραφία αναφέρεται σε αυτό ως `κρυφή μεταβλητή', εδώ όμως χρησιμοποιούμε μια πιο άμεση ονομασία που ταιριάζει στο πλαίσιο του παρόντος κειμένου.} Το $\bm{s}^{(i)}$ για καθένα από τους πίνακες βάρους $\bm{w}^{(i)}$ είναι:
\begin{equation}
	\label{eq:similarity}
	\bm{s}^{(i)} = \bm{x} * \bm{w}^{(i)}
\end{equation}

\noindent
, όπου $*$ είναι η συνέλιξη\footnote{Χρησιμοποιούμε την συνέλιξη αντί της αλληλοσυσχέτισης μόνο για λόγους συμβατότητας με την προηγούμενη βιβλιογραφία και τα υπολογιστικά πλαίσια. Η χρήση αλληλοσυσχέτισης θα παρήγαγε τα ίδια αποτελέσματα και επιπλέον δεν θα απαιτούσε την περιστροφή των πυρήνων κατά τη διάρκεια της οπτικοποίησης.}
Για την διατήρηση του αρχικού μεγέθους του διανύσματος εισόδου συμπληρώνουμε με μηδενικά.
Δεν χρειαζόμαστε όρο μεροληψίας επειδή θα εφαρμοζόταν συνολικά στο $\bm{s}^{(i)}$ κάτι το οποίο είναι σχεδόν ισοδύναμο με την μάθηση της γραμμής βάσης $\bm{x}$.

Έπειτα, περνάμε την $\bm{s}^{(i)}$ και μια παράμετρο αραιότητας $d^{(i)}$ στην συνάρτηση αραιής ενεργοποίησης $\phi$ με αποτέλεσμα τον χάρτη ενεργοποίησης $\bm{\alpha}^{(i)}$:
\begin{equation}
	\label{eq:extrema}
	\bm{\alpha}^{(i)} = \phi(\bm{s}^{(i)}, d^{(i)})
\end{equation}

\noindent
, όπου $\bm{\alpha}^{(i)}$ είναι ένας αραιός πίνακας, του οποίου τα μη-μηδενικά στοιχεία υποδηλώνουν τις χωρικές θέσεις των στιγμιότυπων του $i^{th}$ πυρήνα.
Η ακριβής μορφή του $\phi$ και του $d^{(i)}$ εξαρτώνται από την επιλογή της συνάρτησης αραιής ενεργοποίησης, οι οποίες παρουσιάζονται στην ενότητα~\ref{sec6:safs}.

Συνελλίσουμε κάθε $\bm{\alpha}^{(i)}$ με το αντίστοιχο $\bm{w}^{(i)}$, έχοντας ως αποτέλεσμα ένα σύνολο ατομικών ανακατασκευών $\bm{r}^{(i)}$ της εισόδου:
\begin{equation}
	\label{eq:reconstructions}
	\bm{r}^{(i)} = \bm{\alpha}^{(i)} * \bm{w}^{(i)}
\end{equation}

\noindent
, που αποτελείται από αραιά επαναλαμβανόμενα πρότυπα $\bm{w}^{(i)}$ με μεταβλητό πλάτος.
Τέλος, μπορούμε να ανακατασκευάσουμε την είσοδο ως το άθροισμα των μεμονωμένων ανακατασκευών $\bm{r}^{(i)}$ ως εξής:
\begin{equation}
	\label{eq:output1}
	\hat{\bm{x}} = \sum \limits_{i=1}^q \bm{r}^{(i)}
\end{equation}

Το μέσο απόλυτο σφάλμα (Mean Absolute Error, MAE) της εισόδου $\bm{x}$ και η πρόβλεψη $\hat{\bm{x}}$ υπολογίζεται ως εξής:
\begin{equation}
	\label{eq:lossfunction}
	\mathcal{L}\left( {\bm{x},\hat{\bm{x}}} \right) = \frac{1}{n}\sum\limits_{t=1}^n \left|\hat{\bm{x}}_t - \bm{x}_t \right|
\end{equation}

\noindent
, όπου ο δείκτης $t$ δηλώνει το $t^{th}$ δείγμα.
Η επιλογή της ΜΑΕ βασίζεται στην ανάγκη να αντιμετωπιστούν οι απομακρυσμένες τιμές των δεδομένων με το ίδιο βάρος με τις κανονικές τιμές.
Ωστόσο τα SAN δεν περιορίζονται στη χρήση του MAE αλλά μπορούν να χρησιμοποιήσουν και άλλες συναρτήσεις απώλειας, όπως το μέσο τετραγωνικό σφάλμα.

Χρησιμοποιώντας backpropagation υπολογίζονται οι κλίσεις του σφάλματος απώλειας $\mathcal{L}$ σε σχέση με το $\bm{w}^{(i)}$:
\begin{equation}
	\label{eq:backpropagation1}
	\nabla\mathcal{L} = \left( \frac{\partial\mathcal{L}}{\partial\bm{w}^{(1)}},\ldots,\frac{\partial\mathcal{L}}{\partial\bm{w}^{(q)}}\right)
\end{equation}

Τέλος, το $\bm{w}^{(i)}$ ενημερώνεται χρησιμοποιώντας τον ακόλουθο κανόνα μάθησης:
\begin{equation}
	\label{eq:backpropagation2}
	\Delta\bm{w}^{(i)} = -\lambda\frac{\partial\mathcal{L}}{\partial\bm{w}^{(i)}}
\end{equation}

\noindent
, όπου $\lambda$ είναι ο ρυθμός μάθησης.

Μετά την εκπαίδευση, θεωρούμε τα $\bm{\alpha}^{(i)}$ (τα οποία υπολογίζονται κατά τη διάρκεια της προς-τα-εμπρός διάδοσης από την Εξ.~\ref{eq:extrema}) και τα $\bm{w}^{(i)}$ (τα οποία υπολογίζονται με τη χρήση του backpropagation από την Εξ.~\ref{eq:backpropagation2}) ως τη συμπιεσμένη αναπαράσταση $\bm{x}$, η οποία μπορεί να ανακατασκευαστεί από τις Εξισώσεις~\ref{eq:reconstructions} και~\ref{eq:output1}:
\begin{equation}
	\label{eq:output2}
	\hat{\bm{x}} = \sum\limits_{i=1}^q \left(\bm{\alpha}^{(i)} * \bm{w}^{(i)}\right)
\end{equation}

Όσον αφορά το μέτρο $\varphi$ και παίρνοντας υπόψη την Εξ.~\ref{eq:output2} στόχος μας είναι να υπολογίσουμε μια όσο το δυνατόν ακριβή αναπαράσταση του $\bm{x}$ μέσω των $\bm{\alpha}^{(i)}$ και $\bm{w}^{(i)}$ με τον μικρότερο αριθμό μη-μηδενικών ενεργοποιήσεων και βαρών.

Η γενική διαδικασία εκπαίδευσης των SAN για πολλαπλές εποχές χρησιμοποιώντας παρτίδες (αντί για ένα παράδειγμα όπως παρουσιάστηκε προηγουμένως) παρουσιάζεται στον Αλγόριθμο~\ref{alg:training}.

\begin{algorithm}[H]
	\caption{Εκπαίδευση δικτύων αραιής ενεργοποίησης}
	\label{alg:training}
	\input{chapter6_san_train.tex}
\end{algorithm}


\section{Πειράματα}
\label{sec6:experiments}
Για όλα τα πειράματα τα βάρη των πυρήνων SAN αρχικοποιούνται χρησιμοποιώντας την κανονική κατανομή $\mathcal{N} (\mu, \sigma)$ με $\mu = 0$ και $\sigma = 0.1$.
Χρησιμοποιήσαμε τον Adam~\cite{kingma2014adam} ως βελτιστοποιητή με ρυθμό μάθησης $\lambda = 0.01$, $b_1 = 0.9$, $b_2 = 0.999$, epsilon $\epsilon = 10^{-8}$ χωρίς σταδιακή απομείωση βαρών.
Για την υλοποίηση και την εκπαίδευση των SANs χρησιμοποιήσαμε το Pytorch~\cite{paszke2017automatic}, με NVIDIA Titan X Pascal GPU 12GB RAM και 12 Core Intel i7-8700 CPU @ 3.20GHz σε λειτουργικό σύστημα Linux.

\subsection{Σύγκριση του μέτρου $\varphi$ για συναρτήσεις αραιής ενεργοποίησης και διάφορα μεγέθη πυρήνα στην Physionet}
Εδώ μελετάμε την επίδραση στο $\bar\varphi$, της επιλογής του μεγέθους πυρήνα $m$ και των συναρτήσεων αραιής ενεργοποίησης που ορίστηκαν στην ενότητα~\ref{sec6:safs}.

\subsubsection{Βάσεις δεδομένων}
Χρησιμοποιούμε ένα σήμα από καθεμία από τις 15 βάσεις δεδομένων της Physionet που αναφέρονται στην πρώτη στήλη του Πίνακα~\ref{table:crrl}.
Κάθε σήμα αποτελείται από $12000$ δείγματα τα οποία με τη σειρά τους κατανέμονται σε $12$ σήματα με $1000$ δείγμα το καθένα, για τη δημιουργία των σημάτων εκπαίδευσης ($6$), επικύρωσης ($2$ σήματα) και δοκιμής ($4$ σήματα).
Η μόνη προεπεξεργασία που γίνεται είναι η αφαίρεση του μέσου όρου και διαίρεση με την τυπική απόκλιση στα σήματα των $1000$ δειγμάτων.

\subsubsection{Ρύθμιση πειράματος}
Εκπαιδεύουμε τέσσερα SAN (ένα για κάθε συνάρτηση αραιής ενεργοποίησης) για καθεμία από τις $15$ βάσεις δεδομένων της Physionet για $30$ εποχές με μέγεθος παρτίδας $2$ και μέγεθος πυρήνων που κυμαίνονται στην περιοχή $[1, 250]$.
Κατά τη διάρκεια της επικύρωσης επιλέξαμε τα μοντέλα με μέγεθος πυρήνα που πέτυχαν το καλύτερο $\bar\varphi$ από όλες τις εποχές.
Κατά τη διάρκεια των δοκιμών τροφοδοτούμε τα δεδομένα δοκιμής στο επιλεγμένο μοντέλο και υπολογίζουμε τα $CR^{-1}$, $\tilde{\mathcal{L}}$ και $\bar\varphi$ για αυτό το σύνολο υπερπαραμέτρων όπως φαίνεται στον Πίνακα~\ref{table:crrl}.
Για τη συνάρτηση ενεργοποίησης ακρότατων θέτουμε `συνοριακή ανοχή' τριών δειγμάτων.

\subsubsection{Αποτελέσματα}
Η ύπαρξη των τριών ξεχωριστών ομάδων που απεικονίζονται στην Εικ.~\ref{fig:crrl} και Εικ.~\ref{fig:flethos}\subref{subfig:crrl_density_plot} μεταξύ της Identity, του ReLU και των υπολοίπων είναι το αποτέλεσμα της επίδρασης της αραιότητας των συναρτήσεων ενεργοποίησης στις αναπαραστάσεις.
Όσο αραιότερη η συνάρτηση ενεργοποίησης είναι τόσο περισσότερο συμπιέζεται η αναπαράσταση, μερικές φορές σε βάρος του σφάλματος ανακατασκευής.
Ωστόσο, με οπτική επιθεώρηση της Εικ.\ref{fig:kernelvisualization} μπορούμε να επιβεβαιώσουμε ότι τα βάρη των SANs με αραιότερους χάρτες ενεργοποίησης (Extrema-Pool indices και Extrema) αντιστοιχούν σε επαναλαμβανόμενα μοτίβα των βάσεων δεδομένων, επιτυγχάνοντας έτσι υψηλή ερμηνευσιμότητα.
Αυτά τα αποτελέσματα δείχνουν ότι το σφάλμα ανακατασκευής από μόνο του δεν είναι επαρκές μέτρο για την αποσύνθεση δεδομένων σε ερμηνεύσιμα στοιχεία.
Προσπαθώντας να επιτύχουμε αποκλειστικά μικρότερο σφάλμα ανακατασκευής (όπως στην περίπτωση της Identity) έχουμε ως αποτέλεσμα θορυβώδεις πυρήνες, ενώ το συνδυασμένο μέτρο του σφάλματος ανακατασκευής και του λόγου συμπίεσης (μικρότερο $\bar\varphi$) έχει ως αποτέλεσμα ερμηνεύσιμους πυρήνες.
Συγκρίνοντας τις διαφορές στο $\bar\varphi$ μεταξύ του Identity, του ReLU και των υπόλοιπων συναρτήσεων αραιής ενεργοποίησης στην Εικ.~\ref{fig:flethos}\subref{subfig:flethos_m} παρατηρούμε ότι οι τελευταίες παράγουν μια περιοχή ελαχίστων στην οποία έχουμε ως αποτέλεσμα ερμηνεύσιμους πυρήνες.

\begin{figure}
	\input{chapter6_crrl.tex}
	\caption[Αντίστροφος λόγος συμπίεσης ($CR^{-1}$) έναντι κανονικοποιημένης απώλειας ανακατασκευής ($\tilde{\mathcal{L}}$) για $15$ βάσεις δεδομένων της Physionet και για διάφορα μεγέθη πυρήνα.]{Αντίστροφος λόγος συμπίεσης ($CR^{-1}$) έναντι κανονικοποιημένης απώλειας ανακατασκευής ($\tilde{\mathcal{L}}$) για $15$ βάσεις δεδομένων της Physionet και για διάφορα μεγέθη πυρήνα.
	Οι πέντε μικρές γραφικές παραστάσεις με το κίτρινο φόντο στα δεξιά της κάθε εικόνας, απεικονίζουν τον αντίστοιχο πυρήνα για το μέγεθος του πυρήνα που πέτυχε το καλύτερο $\varphi$.}
	\label{fig:crrl}
\end{figure}

\begin{sidewaysfigure}
	\centering
	\subfloat[Διάγραμμα πυκνότητας $CR^{-1}$ vs. $\tilde{\mathcal{L}}$]{\includegraphics[width=0.32\textwidth]{"images_1d/crrl_density_plot"}\label{subfig:crrl_density_plot}}
	\subfloat[Διαστήματα εμπιστοσύνης $\bar\varphi$ vs. $epochs$]{\includegraphics[width=0.32\textwidth]{"images_1d/mean_flethos_validation_epochs"}\label{subfig:flethos_epochs}}
	\subfloat[Διαστήματα εμπιστοσύνης $\bar\varphi$ vs. $m$]{\includegraphics[width=0.32\textwidth]{"images_1d/mean_flethos_variable_kernel_size_list"}\label{subfig:flethos_m}}
	\caption{Συνολικά αποτελέσματα της αξιολόγησης των βάσεων δεδομένων της Physionet με τη χρήση του μέτρου $\varphi$.
	Το διάγραμμα πυκνότητας δημιουργήθηκε χρησιμοποιώντας εκτίμηση πυκνότητας γκαουσιανών πυρήνων και τα διαστήματα εμπιστοσύνης απεικονίζουν μια τυπική απόκλιση.}
	\label{fig:flethos}
\end{sidewaysfigure}

\begin{sidewaysfigure}
	\centering
	\input{chapter6_kernelvisualization.tex}
	\caption{Οπτικοποίηση των πυρήνων για κάθε αραιή συνάρτηση ενεργοποίησης (γραμμή) και για κάθε βάση δεδομένων της Physionet (στήλη).
	}
	\label{fig:kernelvisualization}
\end{sidewaysfigure}

\begin{sidewaystable}
	\centering
	\caption{Μέγεθος πυρήνα $m$ με το καλύτερο $\varphi$ για κάθε συνάρτηση αραιής ενεργοποίησης για κάθε βάση δεδομένων της Physionet}
	\label{table:crrl}
	\input{table_mean_inverse_compression_ratio_mean_reconstruction_loss_variable_kernel_size.tex}
\end{sidewaystable}

\subsection{Αξιολόγηση της ανακατασκευής των SANs με χρήση επιβλεπώμενου CNN για αναγνώριση επιληψίας στην UCI}
Εδώ μελετάμε την ποιότητα των ανακατασκευών των SAN εκπαιδεύοντας ένα επιτηρούμενο 1D Συνελικτικό Νευρωνικό Δίκτυο (CNN) στις εξόδους του κάθε SAN\@.
Επίσης, μελετάμε την επίδραση που έχει το $m$ στο $\bar\varphi$ και την ακρίβεια του ταξινομητή για τις πέντε συναρτήσεις αραιής ενεργοποίησης.

\subsubsection{Βάση δεδομένων}
Χρησιμοποιούμε τη βάση δεδομένων αναγνώρισης επιληψίας από την UCI που αποτελείται από $500$ σήματα των $4097$ δειγμάτων (23.5 δευτερόλεπτα) το καθένα.
Η βάση δεδομένων απαριθμείται σε πέντε κατηγορίες με $100$ σήματα για κάθε κατηγορία.
Για τους σκοπούς αυτής της εργασίας χρησιμοποιούμε μια παραλλαγή της βάσης δεδομένων\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition}} στην οποία τα EEG σήματα χωρίζονται σε τμήματα των $178$ δειγμάτων το καθένα, με αποτέλεσμα μια ισορροπημένη βάση δεδομένων που αποτελείται από $11500$ σήματα EEG συνολικά.

\subsubsection{Ρύθμιση πειράματος}
Αρχικά, συγχωνεύουμε τις κατηγορίες όγκων ($2$ και $3$) και των ματιών ($4$ και $5$) με αποτέλεσμα μια τροποποιημένη βάση δεδομένων τριών κατηγοριών (όγκος, μάτια, επιληψία).
Στη συνέχεια διαιρούμε τα $11500$ σήματα σε $76\%$, $12\%$ και $12\%$ ($8740,1380,1380$) ως δεδομένα εκπαίδευσης, επικύρωσης και δοκιμής αντίστοιχα και κανονικοποιούμε στο εύρος $[0, 1]$ χρησιμοποιώντας το ολικό μέγιστο και ελάχιστο.
Για τα SAN χρησιμοποιήσαμε δύο πυρήνες $q = 2$ με μεταβλητό μήκος εύρους $[15, 22]$ και εκπαιδεύσαμε για $5$ εποχές με μέγεθος παρτίδας $32$.
Μετά την εκπαίδευση, επιλέγουμε το μοντέλο που εμφάνισε το χαμηλότερο $\bar\varphi$ από όλες τις εποχές.

Κατά τη διάρκεια της επιβλεπώμενης μάθησης, τα βάρη των πυρήνων παγώνουν και ένα CNN στοιβάζεται πάνω από τις ανακατασκευές των SAN\@.
Ο εξαγωγέας χαρακτηριστικών του CNN αποτελείται από δύο συνελικτικά επίπεδα με φίλτρα $3$ και $16$ και μέγεθος πυρήνα $5$, το καθένα από τα οποία ακολουθείται από ένα ReLU και Extrema-Pool με μέγεθος συγκέντρωσης $2$.
Ο ταξινομητής αποτελείται από τρία πλήρως συνδεδεμένα επίπεδα με μονάδες $656$, $120$ και $84$.
Τα πρώτα δύο από τα πλήρως συνδεδεμένα επίπεδα ακολουθούνται από ένα ReLU ενώ το τελευταίο περνάει από ένα log-softmax το οποίο παράγει τις προβλέψεις.
Το CNN εκπαιδεύεται για $5$ πρόσθετες εποχές με το ίδιο μέγεθος παρτίδας και τη διαδικασία επιλογής μοντέλου όπως και με τα SAN και την αρνητική λογαριθμική πιθανοφάνεια ως συνάρτηση απώλειας.
Για τη συνάρτηση ενεργοποίησης ακρότατων θέτουμε `συνοριακή ανοχή' δύο δειγμάτων.

\subsubsection{Αποτελέσματα}
Όπως φαίνεται στον Πίνακα.~\ref{table:uciepilepsysupervised}, αν και χρησιμοποιούμε ένα σημαντικά μειωμένο μέγεθος αναπαράστασης, η ακρίβεια δεν πέφτει ανάλογα κάτι το οποίο δείχνει ότι τα SANs επιλέγουν τα πιο σημαντικά χαρακτηριστικά για να αναπαραστήσουν τα δεδομένα.
Για παράδειγμα, για $m = 15$ για την Peak συνάρτηση ενεργοποίησης, υπάρχει μια πτώση ακρίβειας $1.44\%$ (το CNN βάση αναφοράς με τα αρχικά δεδομένα πέτυχε $\DTLfetch{keys_values}{key}{uci_epilepsy_supervised_accuracy}{value}\%$) παρόλο που χρησιμοποιήθηκε μια μειωμένη αναπαράσταση με μόλις $34\%$ μέγεθος σε σχέση με τα αρχικά δεδομένα.

\begin{sidewaystable}
	\centering
	\caption{SANs με επιβλεπώμενο στοιβαγμένο CNN για αναγνώριση επιληψίας στην UCI}
	\label{table:uciepilepsysupervised}
	\input{table_uci_epilepsy_supervised.tex}
\end{sidewaystable}

\subsection{Αξιολόγηση της ανακατασκευής των SANs με χρήση επιβλεπώμενου FNN στην MNIST και FMNIST}
\subsubsection{Βάση δεδομένων}
Για την ίδια εργασία με την προηγούμενη, αλλά για 2D, χρησιμοποιούμε την MNIST~\cite{lecun1998gradient} η οποία αποτελείται από μια βάση δεδομένων εκπαίδευσης $60000$ χειρόγραφων ψηφίων στην κλίμακα του γκρι και μια βάση δεδομένων δοκιμής με $10000$ εικόνες καθεμία με μέγεθος $28\times 28$.
Η ίδια διαδικασία ακολουθείται και για την FMNIST~\cite{xiao2017fashion}.

\subsubsection{Ρύθμιση πειράματος}
Τα μοντέλα αποτελούνται από δύο πυρήνες $q = 2$ με μεταβλητό μήκος στο εύρος $[1, 6]$.
Χρησιμοποιούμε $10000$ εικόνες από τη βάση δεδομένων εκπαίδευσης για επικύρωση και εκπαιδεύουμε με τα υπόλοιπα $50000$ για $5$ εποχές και μέγεθος παρτίδας $64$.
Δεν εφαρμόζουμε προεπεξεργασία στις εικόνες.

Κατά τη διάρκεια της επιβλεπώμενης μάθησης, τα βάρη των πυρήνων παγώνουν και ένα μονοστρωματικό πλήρως συνδεδεμένο δίκτυο (FNN) στοιβάζεται πάνω από τις τελικές ανακατασκευές των SAN\@.
Το FNN εκπαιδεύεται για $5$ πρόσθετες εποχές με την ίδια διαδικασία επιλογής μοντέλου και μέγεθος παρτίδας όπως και με τα SANs και την αρνητική λογαριθμική πιθανοφάνεια ως συνάρτηση απώλειας.
Για τη συνάρτηση ενεργοποίησης ακρότατων θέτουμε `συνοριακή ανοχή' δύο δειγμάτων.

\subsubsection{Αποτελέσματα}
Όπως φαίνεται στον Πίνακα~\ref{table:mnistsupervised}, η ακρίβεια που επιτυγχάνεται με τις ανακατασκευές ορισμένων SANs είναι συγκρίσιμη με εκείνες ενός FNN που έχει εκπαιδευτεί στα αρχικά δεδομένα ($\DTLfetch{keys_values}{key}{mnist_supervised_accuracy}{value}\%$), αν και έχουν συμπιεστεί σε μεγάλο βαθμό.
Είναι ενδιαφέρον να τονιστεί ότι σε μερικές περιπτώσεις οι ανακατασκευές των SANs, όπως των Extrema-Pool indices, πέτυχαν καλύτερη ακρίβεια από τα αρχικά δεδομένα.
Αυτό υποδηλώνει τη συντριπτική παρουσία περιττής πληροφορίας που βρίσκεται στις αρχικές εικόνες των αρχικών δεδομένων και την δυνατότητα των SANs να εξάγουν τα πιο σημαντικά χαρακτηριστικά από τα δεδομένα.

\begin{sidewaystable}
	\centering
	\caption{SANs με επιβλεπώμενο στοιβαγμένο FNN στην MNIST}
	\label{table:mnistsupervised}
	\input{table_mnist_supervised.tex}
\end{sidewaystable}

\section{Συζήτηση}
\label{sec6:discussion}
Τα SANs σε συνδυασμό με το μέτρο $\varphi$ συμπιέζουν την περιγραφή των δεδομένων σε $\bm{w}^{(i)}$ και $\bm{\alpha}^{(i)}$ κατά παρόμοιο τρόπο με ένα πλαίσιο γλώσσας ελάχιστης περιγραφής (Minimum Description Language).
Τα πειράματα που έγιναν στην ενότητα~\ref{sec6:experiments} δείχνουν ότι η χρήση της ταυτότητας, του ReLU και (σε μικρότερο βαθμό) των Μέγιστων-Ενεργοποιήσεων παράγουν θορυβώδη χαρακτηριστικά, ενώ από την άλλη πλευρά οι δείκτες συγκέντρωσης ακρότατων και τα ακρότατα παράγουν σταθερά χαρακτηριστικά και μπορούν να προσαρμοστούν χρησιμοποιώντας παραμέτρους (μήκος πυρήνα, και $med$) των οποίων οι τιμές μπορούν να καθοριστούν με απλή επισκόπηση των δεδομένων.

Από την πλευρά της μάθησης αραιών λεξιλογίων (Sparse Dictionary Learning), οι πυρήνες των SANs θα μπορούσαν να θεωρηθούν ως άτομα ενός λεξικού που ειδικεύεται στην ερμηνευτική αντιστοίχιση μοτίβων (e.g.\ για είσοδο ECG οι πυρήνες των SAN είναι ECG beats) και ο χάρτης αραιής ενεργοποίησης ως η αναπαράσταση.
Το γεγονός ότι τα SANs είναι ευρύ με λιγότερα και μεγαλύτερα μεγέθη πυρήνα αντί για βαθιά με μικρότερα και περισσότερα μεγέθη πυρήνα, τα καθιστούν πιο ερμηνεύσιμα από τα DNN και σε ορισμένες περιπτώσεις χωρίς να θυσιάζουν σημαντική ακρίβεια.

Ένα πλεονέκτημα των SANs σε σχέση με τους Αραιούς Αυτοκωδικοποιητές (Sparse Autoencoders)~\cite{ng2011sparse} είναι ότι ο περιορισμός της εγγύτητας των ενεργοποιήσεων μπορεί να εφαρμοστεί ατομικά για κάθε διάνυσμα εισόδου σε αντίθεση με τον υπολογισμό της προς-τα-εμπρός διάδοσης όλων των διανυσμάτων εισόδου.
Επιπλέον, τα SANs δημιουργούν ακριβή μηδενικά αντί για περίπου-μηδενικά, κάτι το οποίο μειώνει την προσαρμογή μεταξύ των ενεργοποιήσεων των νευρώνων.

Το $\varphi$ θα μπορούσε να θεωρηθεί ως μια εναλλακτική τυποποίηση του ξυραφιού του Occam~\cite{soklakov2002occam}, όπως η θεωρία του Solomonov για την επαγωγική εξαγωγή~\cite{solomonoff1964formal}, αλλά με μια αιτιοκρατική ερμηνεία αντί για πιθανοτική.
Το κόστος της περιγραφής των δεδομένων μπορεί να θεωρηθεί ότι είναι ανάλογο του αριθμού των βαρών και του αριθμού των μη-μηδενικών ενεργοποιήσεων, ενώ η ποιότητα της περιγραφής είναι ανάλογη με την απώλεια ανακατασκευής.
Το μέτρο $\varphi$ σχετίζεται επίσης με τη θεωρία του rate-distortion~\cite{burger1971rate}, όπου η μέγιστη παραμόρφωση ορίζεται σύμφωνα με την ανθρώπινη αντίληψη, η οποία όμως αναπόφευκτα εισάγει προκατάληψη.
Υπάρχει επίσης σχέση με τον τομέα της συμπιεσμένης ανίχνευσης (Compressed Sensing)~\cite{donoho2006compressed}, στην οποία εκμεταλλευόμαστε την αραιότητα των δεδομένων, επιτρέποντάς μας να τα ανακατασκευάσουμε με λιγότερα δείγματα από αυτό που απαιτείται από το θεώρημα Nyquist-Shannon και τον τομέα της εξαγωγής σταθερών χαρακτηριστικών (Robust Feature Extraction)~\cite{kim2013deep} όπου τα χαρακτηριστικά χρησιμοποιούνται για αντιπροσώπευση των αρχικών δεδομένων.
Οι Olshausen et al.~\cite{olshausen1996emergence} παρουσίασαν μια συνάρτηση βελτιστοποίησης που εξετάζει υποκειμενικά μέτρα της αραιότητας των χαρτών ενεργοποίησης, ωστόσο σε αυτό το έργο χρησιμοποιούμε το άμεσο μέτρο του λόγου συμπίεσης.
Προηγούμενες εργασίες όπως~\cite{zhang2017ecg} χρησιμοποίησαν ένα σταθμισμένο συνδυασμό του αριθμού των νευρώνων, της διαφοράς μεταξύ των διαστημάτων root-mean-squared και ενός συντελεστή συσχέτισης για τη συνάρτηση βελτιστοποίησης ενός FNN ως μέτρο αλλά χωρίς να ληφθεί υπόψη ο αριθμός των μη-μηδενικών ενεργοποιήσεων.

Ένας περιορισμός των SAN είναι η χρήση μεταβλητών πυρήνων μόνο στο πλάτος, το οποίο δεν επαρκεί για πιο περίπλοκα δεδομένα και επίσης δεν αξιοποιείται πλήρως η συμπιεστότητα των δεδομένων.
Μια πιθανή λύση θα ήταν να χρησιμοποιηθεί ένας δειγματολήπτης πλέγματος~\cite{jaderberg2015spatial} για τον πυρήνα, ο οποίος θα του επέτρεπε να μάθει πιο γενικές μεταβολές (όπως κλίμακα) από την απλή μεταβλητότητα πλάτους.
Ωστόσο, οι πρόσθετες ιδιότητες του πυρήνα θα πρέπει να επιλέγονται παίρνοντας υπόψη τις επιπτώσεις στο μέτρο $\varphi$; το μοντέλο θα πρέπει να συμπιεστεί περισσότερο με μειωμένη απώλεια ανακατασκευής.

\clearpage
\bibliography{chapter6.bib}
\bibliographystyle{unsrt}
