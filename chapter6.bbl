\begin{thebibliography}{10}

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436, 2015.

\bibitem{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em nature}, 323(6088):533, 1986.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{graves2013speech}
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In {\em 2013 IEEE international conference on acoustics, speech and
  signal processing}, pages 6645--6649. IEEE, 2013.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{aghasi2017net}
Alireza Aghasi, Afshin Abdi, Nam Nguyen, and Justin Romberg.
\newblock Net-trim: Convex pruning of deep neural networks with performance
  guarantee.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3177--3186, 2017.

\bibitem{lin2017runtime}
Ji~Lin, Yongming Rao, Jiwen Lu, and Jie Zhou.
\newblock Runtime neural pruning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2181--2191, 2017.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{laughlin2003communication}
Simon~B Laughlin and Terrence~J Sejnowski.
\newblock Communication in neuronal networks.
\newblock {\em Science}, 301(5641):1870--1874, 2003.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{bengio1994learning}
Yoshua Bengio, Patrice Simard, Paolo Frasconi, et~al.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock {\em IEEE transactions on neural networks}, 5(2):157--166, 1994.

\bibitem{glorot2011deep}
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
\newblock Deep sparse rectifier neural networks.
\newblock In {\em Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 315--323, 2011.

\bibitem{nair2010rectified}
Vinod Nair and Geoffrey~E Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In {\em Proceedings of the 27th international conference on machine
  learning (ICML-10)}, pages 807--814, 2010.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem{goodfellow2013maxout}
Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua
  Bengio.
\newblock Maxout networks.
\newblock In {\em Proceedings of the 30th International Conference on Machine
  Learning}, volume~28, pages 1319--1327, 17--19 Jun 2013.

\bibitem{makhzani2013k}
Alireza Makhzani and Brendan Frey.
\newblock K-sparse autoencoders.
\newblock {\em arXiv preprint arXiv:1312.5663}, 2013.

\bibitem{bush1996inhibition}
Paul Bush and Terrence Sejnowski.
\newblock Inhibition synchronizes sparsely connected cortical neurons within
  and between columns in realistic network models.
\newblock {\em Journal of computational neuroscience}, 3(2):91--110, 1996.

\bibitem{bengio2015towards}
Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard, and Zhouhan Lin.
\newblock Towards biologically plausible deep learning.
\newblock {\em arXiv preprint arXiv:1502.04156}, 2015.

\bibitem{rehn2007network}
Martin Rehn and Friedrich~T Sommer.
\newblock A network that uses few active neurones to code visual input predicts
  the diverse shapes of cortical receptive fields.
\newblock {\em Journal of computational neuroscience}, 22(2):135--146, 2007.

\bibitem{heiberg2018firing}
Thomas Heiberg, Birgit Kriener, Tom Tetzlaff, Gaute~T Einevoll, and Hans~E
  Plesser.
\newblock Firing-rate models for neurons with a broad repertoire of spiking
  behaviors.
\newblock {\em Journal of computational neuroscience}, 45(2):103--132, 2018.

\bibitem{bizopoulos2019deep}
Paschalis Bizopoulos and Dimitrios Koutsouris.
\newblock Deep learning in cardiology.
\newblock {\em IEEE reviews in biomedical engineering}, 12:168--193, 2019.

\bibitem{simonyan2013deep}
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock {\em arXiv preprint arXiv:1312.6034}, 2013.

\bibitem{zeiler2014visualizing}
Matthew~D Zeiler and Rob Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In {\em European conference on computer vision}, pages 818--833.
  Springer, 2014.

\bibitem{bach2015pixel}
Sebastian Bach, Alexander Binder, Gr{\'e}goire Montavon, Frederick Klauschen,
  Klaus-Robert M{\"u}ller, and Wojciech Samek.
\newblock On pixel-wise explanations for non-linear classifier decisions by
  layer-wise relevance propagation.
\newblock {\em PloS one}, 10(7):e0130140, 2015.

\bibitem{ribeiro2016should}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock Why should i trust you?: Explaining the predictions of any
  classifier.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD international conference
  on knowledge discovery and data mining}, pages 1135--1144. ACM, 2016.

\bibitem{blier2018description}
L{\'e}onard Blier and Yann Ollivier.
\newblock The description length of deep learning models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2216--2226, 2018.

\bibitem{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, Patrick Haffner, et~al.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{ng2011sparse}
Andrew Ng et~al.
\newblock Sparse autoencoder.
\newblock {\em CS294A Lecture notes}, 72(2011):1--19, 2011.

\bibitem{soklakov2002occam}
Andrei~N Soklakov.
\newblock Occamâ€™s razor as a formal basis for a physical theory.
\newblock {\em Foundations of Physics Letters}, 15(2):107--135, 2002.

\bibitem{solomonoff1964formal}
Ray~J Solomonoff.
\newblock A formal theory of inductive inference. part i.
\newblock {\em Information and control}, 7(1):1--22, 1964.

\bibitem{burger1971rate}
T~Burger.
\newblock Rate distortion theory, 1971.

\bibitem{donoho2006compressed}
David~L Donoho et~al.
\newblock Compressed sensing.
\newblock {\em IEEE Transactions on information theory}, 52(4):1289--1306,
  2006.

\bibitem{kim2013deep}
Yelin Kim, Honglak Lee, and Emily~Mower Provost.
\newblock Deep learning for robust feature generation in audiovisual emotion
  recognition.
\newblock In {\em 2013 IEEE international conference on acoustics, speech and
  signal processing}, pages 3687--3691. IEEE, 2013.

\bibitem{olshausen1996emergence}
Bruno~A Olshausen and David~J Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock {\em Nature}, 381(6583):607, 1996.

\bibitem{zhang2017ecg}
Bo~Zhang, Jiasheng Zhao, Xiao Chen, and Jianhuang Wu.
\newblock Ecg data compression using a neural network model based on
  multi-objective optimization.
\newblock {\em PloS one}, 12(10):e0182500, 2017.

\bibitem{jaderberg2015spatial}
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et~al.
\newblock Spatial transformer networks.
\newblock In {\em Advances in neural information processing systems}, pages
  2017--2025, 2015.

\end{thebibliography}
